{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task List 8 - Dirichlet-multinomial model\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import numpy as np\n",
    "from scipy.stats import dirichlet, multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_args_to_np_array(f):\n",
    "    def inner(*args, **kwargs):\n",
    "        args = list(args)\n",
    "        for idx, arg in enumerate(args):\n",
    "            if isinstance(arg, list):\n",
    "                args[idx] = np.array(arg)\n",
    "        for kw_name, kw_val in kwargs.items():\n",
    "            if isinstance(kw_val, list):\n",
    "                kwargs[kw_name] = np.array(kw_val)\n",
    "        return f(*args, **kwargs)\n",
    "    \n",
    "    return inner\n",
    "\n",
    "def norm(l):\n",
    "    return np.array(l) / sum(np.array(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet visualize\n",
    "def xy2bc(xy, corners, tol=1.e-10):\n",
    "    '''Converts 2D Cartesian coordinates to barycentric.'''\n",
    "    midpoints = [(corners[(i + 1) % 3] + corners[(i + 2) % 3]) / 2.0 \\\n",
    "                 for i in range(3)]\n",
    "    \n",
    "    s = [(corners[i] - midpoints[i]).dot(xy - midpoints[i]) / 0.75 \\\n",
    "         for i in range(3)]\n",
    "    return np.clip(s, tol, 1.0 - tol)\n",
    "\n",
    "\n",
    "def draw_pdf_contours(calc_fn, ax):\n",
    "    corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "    triangle = tri.Triangulation(corners[:, 0], corners[:, 1])\n",
    "    \n",
    "    refiner = tri.UniformTriRefiner(triangle)\n",
    "    trimesh = refiner.refine_triangulation(subdiv=4)\n",
    "    pvals = [calc_fn(xy2bc(xy, corners)) for xy in zip(trimesh.x, trimesh.y)]\n",
    "    nlevels = 100\n",
    "    \n",
    "    ax.tricontourf(trimesh, pvals, nlevels, cmap='hot')\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 0.75**0.5)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Implement a Dirichlet-multinomial model for dice tossing problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Implement a method for posterior predictive distribution of a future observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lk(theta, trials):\n",
    "    lk = 1.0\n",
    "    \n",
    "    for theta_k, N_k  in zip(theta, trials):\n",
    "        lk *= theta_k ** N_k \n",
    "    \n",
    "    return lk\n",
    "\n",
    "\n",
    "@all_args_to_np_array\n",
    "def calc_map(alpha, results, dim=3):\n",
    "    _map = (results + alpha - 1) / (sum(results) + sum(alpha) - dim) \n",
    "    return np.round(_map, 4)\n",
    "    \n",
    "    \n",
    "@all_args_to_np_array\n",
    "def calc_mle(results):\n",
    "    _mle = results / sum(results)\n",
    "    return np.round(_mle, 4)\n",
    "\n",
    "\n",
    "@all_args_to_np_array\n",
    "def calc_posterior_predictive(alpha, results):\n",
    "    probs = (alpha + results) / (sum(alpha) + sum(results))\n",
    "    return np.argmax(probs), list(np.round(probs, 4))\n",
    "    \n",
    "\n",
    "@all_args_to_np_array\n",
    "def dir_mult_model(prior_alpha, results, simplex_x):\n",
    "    prior = dirichlet.pdf(x=simplex_x, alpha=prior_alpha)\n",
    "    likelihood = calc_lk(simplex_x, results)\n",
    "    \n",
    "    posterior = prior * likelihood\n",
    "    exact_posterior = dirichlet.pdf(x=simplex_x, alpha=prior_alpha+results)\n",
    "    \n",
    "    return posterior, exact_posterior\n",
    "\n",
    "\n",
    "def visualize_dirichlet_multinomial(prior, results):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.set_title('Prior\\n(dirichlet%s)' % prior)\n",
    "    draw_pdf_contours(lambda x: dirichlet.pdf(x=x, alpha=prior), ax1)\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.set_title('Likelihood\\n(multinomial%s)' % results)\n",
    "    draw_pdf_contours(lambda x: calc_lk(results, x), ax2)\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.set_title('Posterior\\n(dir_mult%s)' % (np.array(results) + np.array(prior)))\n",
    "    draw_pdf_contours(lambda x: dir_mult_model(prior, results, x)[0], ax3)\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.set_title('Posterior\\n(dir_mult_ext%s)' % (np.array(results) + np.array(prior)))\n",
    "    draw_pdf_contours(lambda x: dir_mult_model(prior, results, x)[1], ax4)\n",
    "\n",
    "    print('MAP:', calc_map(prior, results))\n",
    "    print('MLE:', calc_mle(results))\n",
    "    print('Posterior predictive:', calc_posterior_predictive(prior, results))\n",
    "\n",
    "visualize_dirichlet_multinomial(prior=[2, 2, 2],\n",
    "                                results=[200, 100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Implement prediction mechanism for next word in a text using your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.animation as anim\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().replace('\\n', ' ').strip()\n",
    "\n",
    "    \n",
    "def get_lemmas(doc):\n",
    "    proc_doc = nlp(doc)\n",
    "    \n",
    "    lemmas = [tok.lemma_ for tok in proc_doc \n",
    "              if (not tok.is_punct) and (tok.lemma_ != '-PRON-') and (not tok.is_stop)]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def to_bag_of_words(vocabulary, words):\n",
    "    word_counts = []\n",
    "    for w in vocabulary:\n",
    "        word_counts.append(words.count(w))\n",
    "        \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def split_train_test(doc, train_size=0.8):\n",
    "    words = doc.split(' ')\n",
    "    split_index = int(len(words) * train_size)\n",
    "    train_words, test_words = words[:split_index], words[split_index:]\n",
    "    \n",
    "    return ' '.join(train_words), ' '.join(test_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_occurrences_barplot(words, occurrences, ax):\n",
    "    ax.cla()\n",
    "    g = sns.barplot(words, occurrences, ax=ax)\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "        \n",
    "        \n",
    "def visualize_word_occurrences(prior, vocabulary, test_lemmas):\n",
    "    def animate(i, ax1, ax2):\n",
    "        if i == 0:\n",
    "            return\n",
    "        \n",
    "        # Priors\n",
    "        make_word_occurrences_barplot(vocabulary, prior, ax1)\n",
    "        \n",
    "        # Results\n",
    "        results = to_bag_of_words(vocabulary, test_lemmas[:i])\n",
    "        make_word_occurrences_barplot(vocabulary, np.array(prior) + np.array(results), ax2)\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    func_anim = anim.FuncAnimation(fig, animate,\n",
    "                                   frames=list(range(len(test_lemmas) - 1)),\n",
    "                                   fargs=(ax1, ax2), interval=50)\n",
    "    \n",
    "    return func_anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visualization_word_occurrences():\n",
    "    doc = load_document('lyrics.txt')\n",
    "    vocabulary = sorted(set(get_lemmas(doc)))\n",
    "    train_doc, test_doc = split_train_test(doc, train_size=0.2)\n",
    "    \n",
    "    prior = to_bag_of_words(vocabulary, get_lemmas(train_doc))\n",
    "    test_lemmas = get_lemmas(test_doc)\n",
    "    \n",
    "    return visualize_word_occurrences(prior, vocabulary, test_lemmas)\n",
    "        \n",
    "    \n",
    "func_anim = run_visualization_word_occurrences()\n",
    "HTML(func_anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_word_prediction(filepath, lemmatize_fn, train_size=0.2):\n",
    "    doc = load_document(filepath)\n",
    "    vocabulary = sorted(set(lemmatize_fn(doc)))\n",
    "    train_doc, test_doc = split_train_test(doc, train_size=train_size)\n",
    "    \n",
    "    prior = to_bag_of_words(vocabulary, lemmatize_fn(train_doc))\n",
    "    test_lemmas = lemmatize_fn(test_doc)\n",
    "    \n",
    "    prediction_results = {\n",
    "        'correct': 0,\n",
    "        'false': 0\n",
    "    }\n",
    "    \n",
    "    from time import time\n",
    "    last_ts = time()\n",
    "    bow = [0] * len(vocabulary)\n",
    "    \n",
    "    for i in range(len(test_lemmas) - 1):\n",
    "        if i % max(1, int((len(test_lemmas) - 1) / 10)) == 0:\n",
    "            curr_time = time()\n",
    "            interval = np.round((curr_time - last_ts) * 1000, 2)\n",
    "            last_ts = curr_time\n",
    "            print('Step %d of %d' % (i, len(test_lemmas) - 1), 'time: ', interval, '(ms)')\n",
    "        \n",
    "        pred_idx, pred_probs = calc_posterior_predictive(prior, bow)\n",
    "        \n",
    "        if vocabulary[pred_idx] == test_lemmas[i + 1]:\n",
    "            prediction_results['correct'] += 1\n",
    "        else:\n",
    "            prediction_results['false'] += 1\n",
    "            \n",
    "        bow[vocabulary.index(test_lemmas[i])] += 1\n",
    "    \n",
    "    for k, v  in prediction_results.items():\n",
    "        prediction_results[k] = np.round(100 * v / (len(test_lemmas) - 1), 2)\n",
    "            \n",
    "    return prediction_results\n",
    "\n",
    "\n",
    "def run_all_predictions():\n",
    "    filenames = ['lyrics.txt', 'poem.txt', 'story.txt']\n",
    "    lemmatize_fns = [\n",
    "        ('No lemmatization', lambda doc: doc.split(' ')),\n",
    "        ('Spacy lemmatize', lambda doc: get_lemmas(doc)),\n",
    "    ]\n",
    "    results = {}\n",
    "    \n",
    "    for f in filenames:\n",
    "        for fn_name, fn in lemmatize_fns:\n",
    "            print('Prediction for text:', f, 'using', fn_name)\n",
    "            results[(f, fn_name)] = run_word_prediction(f, fn, train_size=0.8)\n",
    "    \n",
    "    pprint(results)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    idx = 1\n",
    "    \n",
    "    for f in filenames:\n",
    "        for fn_name, _ in lemmatize_fns:\n",
    "            ax = fig.add_subplot(len(filenames), 2, idx)\n",
    "            res = results[(f, fn_name)]\n",
    "            g = sns.barplot(list(res.keys()), list(res.values()), ax=ax)\n",
    "            g.text(0, res['correct'] + 0.5, res['correct'], color='black', ha=\"center\")\n",
    "            g.text(1, res['false'] + 0.5, res['false'], color='black', ha=\"center\")\n",
    "            ax.set_title('%s - %s' % (f, fn_name))\n",
    "            \n",
    "            idx += 1\n",
    "    \n",
    "    \n",
    "run_all_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
